# -*- coding: utf-8 -*-
"""nlp_hw3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19MUoFX0-1g-p3esQ1At4aEWiEW4Zr8y0
"""

!pip install transformers
!pip install datasets==2.21.0
!pip install torchmetrics

import transformers as T
from datasets import load_dataset
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from tqdm import tqdm
from torchmetrics import SpearmanCorrCoef, Accuracy, F1Score

# 有些中文的標點符號在tokenizer編碼以後會變成[UNK]，所以將其換成英文標點
token_replacement = [
    ["：" , ":"],
    ["，" , ","],
    ["“" , "\""],
    ["”" , "\""],
    ["？" , "?"],
    ["……" , "..."],
    ["！" , "!"]
]

tokenizer = T.BertTokenizer.from_pretrained("google-bert/bert-base-uncased", cache_dir="./cache/")

class SemevalDataset(Dataset):
    def __init__(self, split="train") -> None:
        super().__init__()
        assert split in ["train", "validation"]
        self.data = load_dataset(
            "sem_eval_2014_task_1", split=split, cache_dir="./cache/", trust_remote_code=True
        ).to_list()

    def __getitem__(self, index):
        d = self.data[index]
        # 把中文標點替換掉
        for k in ["premise", "hypothesis"]:
            for tok in token_replacement:
                d[k] = d[k].replace(tok[0], tok[1])
        return d

    def __len__(self):
        return len(self.data)

data_sample = SemevalDataset(split="train").data[:3]
print(f"Dataset example: \n{data_sample[0]} \n{data_sample[1]} \n{data_sample[2]}")

# Define the hyperparameters
lr = 3e-5
epochs = 3
train_batch_size = 8
validation_batch_size = 8

# TODO1: Create batched data for DataLoader
# `collate_fn` is a function that defines how the data batch should be packed.
# This function will be called in the DataLoader to pack the data batch.

def collate_fn(batch):
    # TODO1-1: Implement the collate_fn function
    # Write your code here
    # The input parameter is a data batch (tuple), and this function packs it into tensors.
    # Use tokenizer to pack tokenize and pack the data and its corresponding labels.
    # Return the data batch and labels for each sub-task.
    premise = [item["premise"] for item in batch]
    hypothesis = [item["hypothesis"] for item in batch]

    # Tokenize the batch
    encoding = tokenizer(premise, hypothesis, padding=True, truncation=True, return_tensors="pt") # max_length=300

    # Label
    labels = ["relatedness_score", "entailment_judgment"]
    label = {}
    for l in labels:
        label[l] = [item[l] for item in batch]
        label[l] = torch.tensor(label[l])

    return encoding, label


# TODO1-2: Define your DataLoader
dl_train = DataLoader(SemevalDataset(split="train"), batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)
dl_validation = DataLoader(SemevalDataset(split="validation"), batch_size=validation_batch_size, shuffle=False, collate_fn=collate_fn)

# print(next(iter(dl_train)))

# # [CLS]: 101
# # [SEP]: 102
# # [PAD]" 0"

# # print encoding
# for batch, label in dl_train:
#     print(batch, label)
#     break

# TODO2: Construct your model
class MultiLabelModel(torch.nn.Module):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Write your code here
        # Define what modules you will use in the model

        # BERT from HuggingFace
        self.bert = T.BertModel.from_pretrained("google-bert/bert-base-uncased", cache_dir="./cache/")

        # Dropout layer
        self.dropout = torch.nn.Dropout(0.3)
        # an effective technique for regularization and preventing the co-adaptation of neurons

        # Two Linear for two outputs
        self.linear1 = torch.nn.Linear(self.bert.config.hidden_size, 1)
        self.linear2 = torch.nn.Linear(self.bert.config.hidden_size, 3)

        # Dynamic weight for calc loss
        self.var1 = torch.nn.Parameter(torch.tensor(0.0))
        self.var2 = torch.nn.Parameter(torch.tensor(0.0))

    def forward(self, **kwargs):
        # Write your code here
        # Forward pass
        input_ids = kwargs.get('input_ids')
        attention_mask = kwargs.get('attention_mask')
        token_type_ids = kwargs.get('token_type_ids')

        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

        # Get the last hidden state
        sequence_output = outputs.last_hidden_state

        # Attention mask
        mask_expanded = attention_mask.unsqueeze(-1).expand(sequence_output.size())

        # # Max pooling
        # pooled_output, _ = torch.max(sequence_output * mask_expanded, 1)

        # Mean pooling
        sum_embeddings = torch.sum(sequence_output * mask_expanded, 1)
        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)
        pooled_output = sum_embeddings / sum_mask

        # Apply dropout
        pooled_output = self.dropout(pooled_output)

        # Forward pass through regression and classification heads
        output1 = self.linear1(pooled_output)
        output2 = self.linear2(pooled_output)

        return output1, output2

device = "cuda" if torch.cuda.is_available() else "cpu"
model = MultiLabelModel().to(device)

# TODO3: Define your optimizer and loss function

# TODO3-1: Define your Optimizer
optimizer = AdamW(model.parameters(), lr=lr)

# TODO3-2: Define your loss functions (you should have two)
loss_fn1 = torch.nn.MSELoss()
loss_fn2 = torch.nn.CrossEntropyLoss()

# scoring functions
spc = SpearmanCorrCoef().to(device)
acc = Accuracy(task="multiclass", num_classes=3).to(device)
f1 = F1Score(task="multiclass", num_classes=3, average='macro').to(device)

for ep in range(epochs):
    pbar = tqdm(dl_train)
    pbar.set_description(f"Training epoch [{ep+1}/{epochs}]")
    model.train()
    # TODO4: Write the training loop
    # Write your code here
    # train your model
    i = 0
    for batch, label in pbar:
        batch = {k: v.to(device) for k, v in batch.items()}
        label = {k: v.to(device) for k, v in label.items()}

        # clear gradient
        optimizer.zero_grad()

        # forward pass
        output1, output2 = model(**batch)

        # # print size of output
        # print(output1.size())
        # print(output2.size())

        # compute loss
        loss1 = loss_fn1(output1, label['relatedness_score'])
        loss2 = loss_fn2(output2, label['entailment_judgment'])
        loss1 = loss1 / (2 * torch.exp(model.var1)) + model.var1
        loss2 = loss2 / (2 * torch.exp(model.var2)) + model.var2
        loss = loss1 + loss2

        # back-propagation
        loss.backward()

        # # Gradient clipping
        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1)

        # model optimization
        optimizer.step()

        # Print loss to tqdm every 50 iter
        if i % 50 == 0:
            pbar.set_postfix({"loss": loss.item()})
        i += 1

    pbar = tqdm(dl_validation)
    pbar.set_description(f"Validation epoch [{ep+1}/{epochs}]")
    model.eval()
    # TODO5: Write the evaluation loop
    # Write your code here
    # Evaluate your model
    # Output all the evaluation scores (SpearmanCorrCoef, Accuracy, F1Score)

    with torch.no_grad():
        spc.reset()
        acc.reset()
        f1.reset()

        for batch, label in pbar:
            batch = {k: v.to(device) for k, v in batch.items()}
            label = {k: v.to(device) for k, v in label.items()}

            # Forward pass
            output1, output2 = model(**batch)

            # Compute metrics
            spc.update(output1.squeeze(-1), label['relatedness_score'])
            acc.update(output2, label['entailment_judgment'])
            f1.update(output2, label['entailment_judgment'])

    # Print spc, acc, f1
    print(f"Spearman Corr: {spc.compute()}")
    print(f"Accuracy: {acc.compute()}")
    print(f"F1 Score: {f1.compute()}")

    torch.save(model, f'./ep{ep}.ckpt')

# model = torch.load('./saved_models/ep2.ckpt')
# model.eval()

# # Define premises and hypotheses
# pairs = [
#     ("The boy is playing soccer.", "The boy is engaging in a sport."),
#     ("She is baking a cake.", "She is making a dessert."),
#     ("The cat is sleeping on the mat.", "The cat is playing with a toy."),
#     ("He is swimming in the pool.", "He is sitting on the couch."),
#     ("The weather is nice today.", "I am going to the market."),
#     ("She loves reading books.", "He is fixing his bike.")
# ]

# # Process the pairs
# for premise, hypothesis in pairs:
#     inputs = tokenizer(premise, hypothesis, padding=True, truncation=True, return_tensors="pt")
#     inputs = {k: v.to(device) for k, v in inputs.items()}

#     with torch.no_grad():
#         outputs = model(**inputs)
#         print(outputs)
#         output1, output2 = outputs
#         predicted_relatedness = output1.item()
#         predicted_entailment = torch.argmax(output2, dim=1).item()
#         print(f"Premise: {premise}")
#         print(f"Hypothesis: {hypothesis}")
#         print(f"Predicted Relatedness: {predicted_relatedness}")
#         print(f"Predicted Entailment: {predicted_entailment}")
#         print("-" * 50)

"""For test set predictions, you can write perform evaluation simlar to #TODO5."""