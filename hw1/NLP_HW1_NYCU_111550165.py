# -*- coding: utf-8 -*-
"""nlp_hw1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CRWYhjxVSEMksSiD6y-WSDOlZJZJf2Qc

## Part I: Data Pre-processing
"""

import pandas as pd

# Download the Google Analogy dataset
!wget http://download.tensorflow.org/data/questions-words.txt

# Preprocess the dataset
file_name = "questions-words"
with open(f"{file_name}.txt", "r") as f:
    data = f.read().splitlines()

# check data from the first 10 entries
for entry in data[:10]:
    print(entry)

# TODO1: Write your code here for processing data to pd.DataFrame
# Please note that the first five mentions of ": " indicate `semantic`,
# and the remaining nine belong to the `syntatic` category.

questions = []
categories = []
sub_categories = []
cat_count = 0
cat_cur = "Semantic"
sub_cat_cur = "capital-common-countries"

for entry in data:
    if cat_count > 5:
        cat_cur = "Syntactic"
    if entry[0] == ':':
        cat_count+=1
        sub_cat_cur = entry
        continue

    # input data
    questions.append(entry)
    categories.append(cat_cur)
    sub_categories.append(sub_cat_cur)

# Create the dataframe
df = pd.DataFrame(
    {
        "Question": questions,
        "Category": categories,
        "SubCategory": sub_categories,
    }
)

df.head()

df.to_csv(f"{file_name}.csv", index=False)

"""## Part II: Use pre-trained word embeddings
- After finish Part I, you can run Part II code blocks only.
"""

import pandas as pd
import numpy as np
import gensim.downloader
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

data = pd.read_csv("questions-words.csv")

MODEL_NAME = "glove-wiki-gigaword-100"
# You can try other models.
# https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models

# Load the pre-trained model (using GloVe vectors here)
model = gensim.downloader.load(MODEL_NAME)
print("The Gensim model loaded successfully!")

# Do predictions and preserve the gold answers (word_D)
preds = []
golds = []

for analogy in tqdm(data["Question"]):
    # TODO2: Write your code here to use pre-trained word embeddings for getting predictions of the analogy task.
    # You should also preserve the gold answers during iterations for evaluations later.
    """ Hints
    # Unpack the analogy (e.g., "man", "woman", "king", "queen")
    # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
    # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#L776
    # Mikolov et al., 2013: big - biggest and small - smallest
    # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).
    """
    # unpack
    words = analogy.lower().split()

    # word_b + word_c - word_a should be close to word_d
    word_d = model[words[1]] + model[words[2]] - model[words[0]]

    # Returns list of array
    pred = model.most_similar(positive=[words[1], words[2]], negative=[words[0]], topn=1)[0][0]

    if pred is None:
        continue

    # Append prediction and gold answer
    preds.append(pred)
    golds.append(words[3])

# Perform evaluations. You do not need to modify this block!!

def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:
    return np.mean(gold == pred)

golds_np, preds_np = np.array(golds), np.array(preds)
data = pd.read_csv("questions-words.csv")

# Evaluation: categories
for category in data["Category"].unique():
    mask = data["Category"] == category
    golds_cat, preds_cat = golds_np[mask], preds_np[mask]
    acc_cat = calculate_accuracy(golds_cat, preds_cat)
    print(f"Category: {category}, Accuracy: {acc_cat * 100}%")

# Evaluation: sub-categories
for sub_category in data["SubCategory"].unique():
    mask = data["SubCategory"] == sub_category
    golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]
    acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)
    print(f"Sub-Category{sub_category}, Accuracy: {acc_subcat * 100}%")

# Collect words from Google Analogy dataset
SUB_CATEGORY = ": family"

# TODO3: Plot t-SNE for the words in the SUB_CATEGORY `: family`
data_family = data[data["SubCategory"] == SUB_CATEGORY]

words_unique = set()
for question in data_family["Question"]:
    words_unique.update(question.lower().split())

words_list = []
word_embeddings = []
for word in words_unique:
    if word in model:
        word_embeddings.append(model[word])
        words_list.append(word)
word_embeddings = np.array(word_embeddings)

# Perform t-SNE
tsne = TSNE(n_components=2, random_state=42, perplexity=12)
tsne_family = tsne.fit_transform(word_embeddings)

# Plotting the results
plt.figure(figsize=(10, 8))
plt.scatter(tsne_family[:, 0], tsne_family[:, 1])

# label
for i, word in enumerate(words_list):
    plt.annotate(word, (tsne_family[i, 0], tsne_family[i, 1]), fontsize=8)

plt.title("Word Relationships from Google Analogy Task")
plt.show()
plt.savefig("word_relationships.png", bbox_inches="tight")

"""### Part III: Train your own word embeddings

### Get the latest English Wikipedia articles and do sampling.
- Usually, we start from Wikipedia dump (https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2). However, the downloading step will take very long. Also, the cleaning step for the Wikipedia corpus ([`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus)) will take much time. Therefore, we provide cleaned files for you.
"""

# # Download the split Wikipedia files
# # Each file contain 562365 lines (articles).
!gdown --id 1jiu9E1NalT2Y8EIuWNa1xf2Tw1f1XuGd -O wiki_texts_part_0.txt.gz
!gdown --id 1ABblLRd9HXdXvaNv8H9fFq984bhnowoG -O wiki_texts_part_1.txt.gz
!gdown --id 1z2VFNhpPvCejTP5zyejzKj5YjI_Bn42M -O wiki_texts_part_2.txt.gz
!gdown --id 1VKjded9BxADRhIoCzXy_W8uzVOTWIf0g -O wiki_texts_part_3.txt.gz
!gdown --id 16mBeG26m9LzHXdPe8UrijUIc6sHxhknz -O wiki_texts_part_4.txt.gz
# !gdown --id 1J0os1846PQ129t720aI0wMm-5GepEwSl -O wiki_texts_part_0.txt.gz
# !gdown --id 1tsI3RSKPN3b2-1IZ0N7bmjgVRf-THIkW -O wiki_texts_part_1.txt.gz
# !gdown --id 1koiw6RFNzDe6pe2zMTfVhsEKmpmnYyu5 -O wiki_texts_part_2.txt.gz
# !gdown --id 1YSGbDqhbg2xJsWD_hYQ5z9URl0dCTC2m -O wiki_texts_part_3.txt.gz
# !gdown --id 1PA3C99C8CcLFjkenT0a9iU07XEQmXyG_ -O wiki_texts_part_4.txt.gz

# # Download the split Wikipedia files
# # Each file contain 562365 lines (articles), except the last file.
!gdown --id 17JFvxOH-kc-VmvGkhG7p3iSZSpsWdgJI -O wiki_texts_part_5.txt.gz
!gdown --id 19IvB2vOJRGlrYulnTXlZECR8zT5v550P -O wiki_texts_part_6.txt.gz
!gdown --id 1sjwO8A2SDOKruv6-8NEq7pEIuQ50ygVV -O wiki_texts_part_7.txt.gz
!gdown --id 1s7xKWJmyk98Jbq6Fi1scrHy7fr_ellUX -O wiki_texts_part_8.txt.gz
!gdown --id 17eQXcrvY1cfpKelLbP2BhQKrljnFNykr -O wiki_texts_part_9.txt.gz
!gdown --id 1J5TAN6bNBiSgTIYiPwzmABvGhAF58h62 -O wiki_texts_part_10.txt.gz
# !gdown --id 1sSLea4hq6Z7oT6noOU_II1ahWjNOKcDX -O wiki_texts_part_5.txt.gz
# !gdown --id 1i6kXTDtZkRiivJ0mj-5GkVbE4gMFlmSb -O wiki_texts_part_6.txt.gz
# !gdown --id 1ain2DN1nxXfsmJ2Aj9TFZlLVJSPsu9Jb -O wiki_texts_part_7.txt.gz
# !gdown --id 1UKhvielQDqQz5pMZ7J3SHv9m8_8gO-dE -O wiki_texts_part_8.txt.gz
# !gdown --id 1q1zMA4hbMS7tID2GTQx-c94UPB8YQaaa -O wiki_texts_part_9.txt.gz
# !gdown --id 1-kkGxwMxPsoGg5_2pdaOeE3Way6njLpH -O wiki_texts_part_10.txt.gz

# Extract the downloaded wiki_texts_parts files.
!gunzip -k wiki_texts_part_*.gz
# !unzip wiki_sampled_zip.zip

# Combine the extracted wiki_texts_parts files.
!cat wiki_texts_part_*.txt > wiki_texts_combined.txt

# Check the first ten lines of the combined file
!head -n 10 wiki_texts_combined.txt

"""Please note that we used the default parameters of [`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus) for cleaning the Wiki raw file. Thus, words with one character were discarded."""

# Now you need to do sampling because the corpus is too big.
# You can further perform analysis with a greater sampling ratio.

import random

wiki_txt_path = "wiki_texts_combined.txt"
output_path = "wiki_sampled.txt"
ratio = 0.20

# wiki_texts_combined.txt is a text file separated by linebreaks (\n).
# Each row in wiki_texts_combined.txt indicates a Wikipedia article.

with open(wiki_txt_path, "r", encoding="utf-8") as f:
    num_lines = sum(1 for _ in tqdm(f))

sampled_articles = set(random.sample(range(num_lines), int(num_lines * ratio)))

with open(wiki_txt_path, "r", encoding="utf-8") as f:
    with open(output_path, "w", encoding="utf-8") as output_file:
    # TODO4: Sample `20%` Wikipedia articles
        for i, article in tqdm(enumerate(f)):
            # Always include articles containing important words
            if i in sampled_articles:
                output_file.write(article)

# import zipfile

# output_zip_name = "wiki_sampled_zip.zip"
# files_to_zip = ['wiki_sampled.txt']
# with zipfile.ZipFile(output_zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:
#     for file in files_to_zip:
#         zipf.write(file)

# TODO5: Train your own word embeddings with the sampled articles
# https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec
# Hint: You should perform some pre-processing before training.

from collections import defaultdict
from gensim.models import Word2Vec
import pandas as pd
import multiprocessing
from gensim.utils import simple_preprocess

# join questions-words.csv with wiki_sampled.txt
wiki_data = pd.read_csv("wiki_sampled.txt")

# Stop words
stoplist = set('the who how what when where and it to a for be to of that on do as is was at this an if I i'.split(' ')) # set to use 'not in'

# Lowercase, split by white space and filter out stopwords
texts = [[word for word in line.lower().split() if word not in stoplist]
         for line in wiki_data]

# Count word freq
frequency = defaultdict(int)
for text in texts:
    for token in text:
        frequency[token] += 1

# Only keep words that appear more than once
processed_corpus = [[word for word in text if frequency[word] > 3] for text in texts]
processed_corpus[0:5]

# init Word2Vec model
model2 = Word2Vec(sentences=processed_corpus,
                  min_count=3, # Ignore words with frequency below this
                  window=10, # Maximum distance between current and predicted word
                  vector_size=300, # word vector dimension
                  workers=multiprocessing.cpu_count()-1)  # Use all available CPU cores
                #   epochs=30)  # Number of iterations over the corpus

# # build vocab
# model2.build_vocab()
# # train model
# model2.train(processed_corpus, total_examples=model2.corpus_count, epochs=30, report_delay=1)

data = pd.read_csv("questions-words.csv")

# Do predictions and preserve the gold answers (word_D)
preds = []
golds = []

for analogy in tqdm(data["Question"]):
    # TODO6: Write your code here to use your trained word embeddings for getting predictions of the analogy task.
    # You should also preserve the gold answers during iterations for evaluations later.
    """ Hints
    # Unpack the analogy (e.g., "man", "woman", "king", "queen")
    # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
    # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/model2s/keyedvectors.py#L776
    # Mikolov et al., 2013: big - biggest and small - smallest
    # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).
    """
    # unpack
    words = analogy.lower().split()

    # word_b + word_c - word_a should be close to word_d
    try:
        pred = model2.wv.most_similar(positive=[words[1], words[2]], negative=[words[0]], topn=1)[0][0]
    except KeyError:
        pred = None

    if pred is not None:
        # Append prediction and gold answer
        preds.append(pred)
        golds.append(words[3])

print(preds[:10])
print(golds[:10])

# Perform evaluations. You do not need to modify this block!!

def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:
    return np.mean(gold == pred)

golds_np, preds_np = np.array(golds), np.array(preds)
data = pd.read_csv("questions-words.csv")

# Evaluation: categories
for category in data["Category"].unique():
    mask = data["Category"] == category
    golds_cat, preds_cat = golds_np[mask], preds_np[mask]
    acc_cat = calculate_accuracy(golds_cat, preds_cat)
    print(f"Category: {category}, Accuracy: {acc_cat * 100}%")

# Evaluation: sub-categories
for sub_category in data["SubCategory"].unique():
    mask = data["SubCategory"] == sub_category
    golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]
    acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)
    print(f"Sub-Category{sub_category}, Accuracy: {acc_subcat * 100}%")

# Collect words from Google Analogy dataset
SUB_CATEGORY = ": family"

# TODO7: Plot t-SNE for the words in the SUB_CATEGORY `: family`
data_family = data[data["SubCategory"] == SUB_CATEGORY]

# Collect unique words from the family category
words_unique = set()
for question in data_family["Question"]:
    words_unique.update(question.lower().split())

# Get word vectors for family words
words_list = []
word_embeddings = []
for word in words_unique:
    if word in model2.wv:
        word_embeddings.append(model2.wv[word])
        words_list.append(word)
    else:
        print(f"Word '{word}' not found in the model.")

# Convert to numpy array
word_embeddings = np.array(word_embeddings)
len(word_embeddings)

# Perform t-SNE
tsne = TSNE(n_components=2, random_state=42, perplexity=1)
tsne_family = tsne.fit_transform(word_embeddings)

# Plotting the results
plt.figure(figsize=(10, 8))
plt.scatter(tsne_family[:, 0], tsne_family[:, 1])

# label
for i, word in enumerate(words_list):
    plt.annotate(word, (tsne_family[i, 0], tsne_family[i, 1]), fontsize=8)

plt.title("Word Relationships from Google Analogy Task")
plt.show()
plt.savefig("word_relationships.png", bbox_inches="tight")